{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashleighperez/DS3010_A/blob/srisaranya/Case_Studies/CaseStudy3_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3AaPphVQtfQ"
      },
      "source": [
        "# Case Study 3 : Textual analysis of movie reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xFcGCJQtfS"
      },
      "source": [
        "**Due Date: December 5, 2023, BEFORE the beginning of class at 12:00pm ET**\n",
        "\n",
        "NOTE: There are always last minute issues submitting the case studies. DO NOT WAIT UNTIL THE LAST MINUTE!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W96Ki3enQtfT"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*o-qaS9WPD9ocA9Ofr85v5g.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z6U9nIKQtfT"
      },
      "source": [
        "**TEAM Members:** Please EDIT this cell and add the names of all the team members in your team\n",
        "\n",
        "  Ashleigh Perez\n",
        "    \n",
        "  Diana Binney\n",
        "    \n",
        "  Srisaranya Pujari"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeewMOzEQtfT"
      },
      "source": [
        "**Desired outcome of the case study.**\n",
        "* In this case study we will look at movie reviews from the v2.0 polarity dataset comes from\n",
        "the http://www.cs.cornell.edu/people/pabo/movie-review-data.\n",
        "    * It contains written reviews of movies divided into positive and negative reviews.\n",
        "* As in Case Study 2 idea is to *analyze* the data set, make *conjectures*, support or refute those conjectures with *data*, and *tell a story* about the data!\n",
        "    \n",
        "**Required Readings:**\n",
        "* This case study will be based upon the scikit-learn Python library\n",
        "* We will build upon the tutorial \"Working With Text Data\" which can be found at http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
        "* In particular, this case study is quite similar to \"Exercise 2: Sentiment Analysis on movie reviews\" on the above web page.\n",
        "* Read about deep learning at https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
        "\n",
        "\n",
        "**Case study assumptions:**\n",
        "* You have access to a python installation\n",
        "\n",
        "**Required Python libraries:**\n",
        "* Numpy (www.numpy.org) (should already be installed from Case Study 2)\n",
        "* Matplotlib (matplotlib.org) (should already be installed from Case Study 2)\n",
        "* Scikit-learn (scikit-learn.org).\n",
        "* You are also welcome to use the Python Natural Language Processing Toolkit (www.nltk.org) (though it is not required).\n",
        "\n",
        "** NOTE **\n",
        "* Please don't forget to save the notebook frequently when working in IPython Notebook, otherwise the changes you made can be lost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQQrXatF30aY"
      },
      "source": [
        "# Getting the data onto Colab example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ludq2Spg3zdC",
        "outputId": "d0869197-d60a-45c7-c6d8-80bfb9a94c1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-30 21:43:18--  https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\n",
            "Resolving www.cs.cornell.edu (www.cs.cornell.edu)... 132.236.207.36\n",
            "Connecting to www.cs.cornell.edu (www.cs.cornell.edu)|132.236.207.36|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3127238 (3.0M) [application/x-gzip]\n",
            "Saving to: ‘review_polarity.tar.gz’\n",
            "\n",
            "review_polarity.tar 100%[===================>]   2.98M  14.5MB/s    in 0.2s    \n",
            "\n",
            "2023-11-30 21:43:18 (14.5 MB/s) - ‘review_polarity.tar.gz’ saved [3127238/3127238]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6lyvK0T4HCk"
      },
      "source": [
        "Look for the directory txt_sentoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QqAfVpEJ4A0P",
        "outputId": "8a1388d1-45b2-45a1-d690-f9790c1d5e5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "poldata.README.2.0  review_polarity.tar.gz  sample_data  txt_sentoken\n"
          ]
        }
      ],
      "source": [
        "! tar xzf review_polarity.tar.gz\n",
        "! ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m-jzLoyQtfU"
      },
      "source": [
        "## Problem 1 (10 points): Complete Exercise 2: Sentiment Analysis on movie reviews from http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyZeq4COQtfU"
      },
      "source": [
        "* Installing scikit-learn using Anaconda does not necessarily download the example source-code.\n",
        "* Accordingly, you may need to download these directly from Github at https://github.com/scikit-learn/scikit-learn:\n",
        "    * The data can be downloaded using doc/tutorial/text_analytics/data/movie_reviews/fetch_data.py\n",
        "    * A skeleton for the solution can be found in doc/tutorial/text_analytics/skeletons/exercise_02_sentiment.py\n",
        "    * A completed solution can be found in doc/tutorial/text_analytics/solutions/exercise_02_sentiment.py\n",
        "* Here is a direct link to the code to help you out:  https://github.com/scikit-learn/scikit-learn/tree/main/doc/tutorial/text_analytics\n",
        "* **It is ok to use the solution provided in the scikit-learn distribution as a starting place for your work.**\n",
        "\n",
        "### Modify the solution to Exercise 2 so that it can run in this iPython notebook\n",
        "* This will likely involve moving around data files and/or small modifications to the script."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "x9rv82MAQtfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "outputId": "592fd905-e1e7-446d-d726-e2f5f6cac265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_samples: 2000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 params - {'vect__ngram_range': (1, 1)}; mean - 0.83; std - 0.02\n",
            "1 params - {'vect__ngram_range': (1, 2)}; mean - 0.86; std - 0.01\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.89      0.89      0.89       255\n",
            "         pos       0.89      0.89      0.89       245\n",
            "\n",
            "    accuracy                           0.89       500\n",
            "   macro avg       0.89      0.89      0.89       500\n",
            "weighted avg       0.89      0.89      0.89       500\n",
            "\n",
            "[[227  28]\n",
            " [ 28 217]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 480x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#----------------------------------------------\n",
        "import sys\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # NOTE: we put the following in a 'if __name__ == \"__main__\"' protected\n",
        "    # block to be able to use a multi-core grid search that also works under\n",
        "    # Windows, see: http://docs.python.org/library/multiprocessing.html#windows\n",
        "    # The multiprocessing module is used as the backend of joblib.Parallel\n",
        "    # that is used when n_jobs != 1 in GridSearchCV\n",
        "\n",
        "    # the training data folder must be passed as first argument\n",
        "    movie_reviews_data_folder = \"txt_sentoken\"\n",
        "    dataset = load_files(movie_reviews_data_folder, shuffle=False)\n",
        "    print(\"n_samples: %d\" % len(dataset.data))\n",
        "\n",
        "    # split the dataset in training and test set:\n",
        "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
        "        dataset.data, dataset.target, test_size=0.25, random_state=None)\n",
        "\n",
        "    # TASK: Build a vectorizer / classifier pipeline that filters out tokens\n",
        "    # that are too rare or too frequent\n",
        "    pipeline = Pipeline([\n",
        "        ('vect', TfidfVectorizer(min_df=3, max_df=0.95)),\n",
        "        ('clf', LinearSVC(C=1000)),\n",
        "    ])\n",
        "\n",
        "    # TASK: Build a grid search to find out whether unigrams or bigrams are\n",
        "    # more useful.\n",
        "    # Fit the pipeline on the training set using grid search for the parameters\n",
        "    parameters = {\n",
        "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "    }\n",
        "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1)\n",
        "    grid_search.fit(docs_train, y_train)\n",
        "\n",
        "    # TASK: print the mean and std for each candidate along with the parameter\n",
        "    # settings for all the candidates explored by grid search.\n",
        "    n_candidates = len(grid_search.cv_results_['params'])\n",
        "    for i in range(n_candidates):\n",
        "        print(i, 'params - %s; mean - %0.2f; std - %0.2f'\n",
        "                 % (grid_search.cv_results_['params'][i],\n",
        "                    grid_search.cv_results_['mean_test_score'][i],\n",
        "                    grid_search.cv_results_['std_test_score'][i]))\n",
        "\n",
        "    # TASK: Predict the outcome on the testing set and store it in a variable\n",
        "    # named y_predicted\n",
        "    y_predicted = grid_search.predict(docs_test)\n",
        "\n",
        "    # Print the classification report\n",
        "    print(metrics.classification_report(y_test, y_predicted,\n",
        "                                        target_names=dataset.target_names))\n",
        "\n",
        "    # Print and plot the confusion matrix\n",
        "    cm = metrics.confusion_matrix(y_test, y_predicted)\n",
        "    print(cm)\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.matshow(cm)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1cVyJKHQtfV"
      },
      "source": [
        "## Problem 2 (10 points): Explore the scikit-learn TfidVectorizer class\n",
        "\n",
        "**Read the documentation for the TfidVectorizer class at http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html.**\n",
        "* Define the term frequency–inverse document frequency (TF-IDF) statistic (http://en.wikipedia.org/wiki/Tf%E2%80%93idf will likely help).\n",
        "* Run the TfidVectorizer class on the training data above (docs_train).\n",
        "* Explore the min_df and max_df parameters of TfidVectorizer.  What do they mean? How do they change the features you get?\n",
        "* Explore the ngram_range parameter of TfidVectorizer.  What does it mean? How does it change the features you get? (Note, large values  of ngram_range may take a long time to run!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "YGCOW1yjQtfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25345a79-3c80-417e-975e-e030e2225267"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#----------------------------------------------\n",
        "\n",
        "# The term frequency-inverse document frequency or TF-IDF statistic is the product of the term frequency statistic\n",
        "# (or how many times a term appears in a document) and the inverse document frequency statistic (or how much information the term provides).\n",
        "# Information via https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(docs_train)\n",
        "vectorizer.get_feature_names_out()\n",
        "\n",
        "vectorizer.min_df # min = 1 (default value)\n",
        "# min_df means if the term appears in the document an amount of times less than the entered or default value (in proportion or in absolute counts) to the document then it will be ignored\n",
        "# Information via https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\n",
        "vectorizer.max_df # max = 1.0 (default value)\n",
        "# max_df means if the term appears in the document an amount of time greater than the entered or default value (in proportion or in absolute counts) to the document then it will be ignored\n",
        "# Information via https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVWQAJmsQtfW"
      },
      "source": [
        "## Problem 3 (15 points): Machine learning algorithms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNt6Ue6-QtfW"
      },
      "source": [
        "* Based upon Problem 2 pick some parameters for TfidfVectorizer\n",
        "    * \"fit\" your TfidfVectorizer using docs_train\n",
        "    * Compute \"Xtrain\", a Tf-idf-weighted document-term matrix using the transform function on docs_train\n",
        "    * Compute \"Xtest\", a Tf-idf-weighted document-term matrix using the transform function on docs_test\n",
        "    * Note, be sure to use the same Tf-idf-weighted class (**\"fit\" using docs_train**) to transform **both** docs_test and docs_train\n",
        "* Examine two classifiers provided by scikit-learn\n",
        "    * LinearSVC\n",
        "    * KNeighborsClassifier\n",
        "    * Why do you think it might be working better?\n",
        "* For a particular choice of parameters and classifier, look at 2 examples where the prediction was incorrect.\n",
        "    * Can you conjecture on why the classifier made a mistake for this prediction?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "m945iZIxQtfW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "c3d71d86-61b4-419b-8ef4-e4fbd2c82939"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-80fadbf9da8d>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msklearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#LinearSVC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sklearn' is not defined"
          ]
        }
      ],
      "source": [
        "#----------------------------------------------\n",
        "vectorizer.fit(docs_train)\n",
        "\n",
        "Xtrain = vectorizer.transform(docs_train)\n",
        "Xtest = vectorizer.transform(docs_test)\n",
        "\n",
        "sklearn = sklearn()\n",
        "\n",
        "#LinearSVC\n",
        "linSVC = sklearn.svm.LinearSVC(docs_train)\n",
        "\n",
        "#KNeighborsClassifier\n",
        "kNeighbors = sklearn.neighbors.KNeighborsClassifier(docs_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoicR0YeQx5x"
      },
      "source": [
        "## Problem 4 (15 points): Using pre-trained models trained from Hugging Face\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Installing some necessary packages for this problem"
      ],
      "metadata": {
        "id": "Ss9kstM7vOq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install transformers[torch]\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "q_IxebKFg4U6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "012b9411-48c6-4630-fe61-05c64e8348d5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers[torch])\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers[torch]) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.24.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.19.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets>=2.0.0->evaluate)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.8.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, responses, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.15.0 dill-0.3.7 evaluate-0.4.1 multiprocess-0.70.15 pyarrow-hotfix-0.6 responses-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking That The Notebook Has a GPU.**\n",
        "\n",
        "#### **This is very important, as the fine-tuning will take very long, or the notebook may even crash, without having a GPU. Do not continue with the rest of problem 4 without seeing \"Sucess!\" in the printout from the cell below.**"
      ],
      "metadata": {
        "id": "Bh9Ns33pvUG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "      device = torch.device(\"cuda:0\")\n",
        "      print(f\"Success!\\nUsing GPU: ({torch.cuda.get_device_name(device=device)})\")\n",
        "else:\n",
        "      print(\"\\nNo GPU found.\\n\\nDO NOT CONTINUE TO PROBLEM 4 or 5 UNTIL YOU SEE \\\"Success!\\\" PRINTED OUT.\\n\\nDo the following:\\n1. Save your colab file.\\n2. Click on Runtime -> Change runtime type -> T4 GPU -> OK (Don't worry about losing progress if the runtime needs to restart, we just saved the file in step #1) -> Save.\\n3. Wait 5-10 sec.\\n5. Click on Runtime -> View Resources. You should see \\\"GPU RAM\\\" as one of the charts.\\n5. Rerun this cell.\")\n",
        "      device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "EIooYQflfZ_y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0a32f26-59b3-45a9-89ad-e0b68d8b8038"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success!\n",
            "Using GPU: (Tesla T4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Using Hugging Face Models \"Off-The-Shelf\"\n",
        "\n",
        "Go to [hugging face's model hub](https://huggingface.co/models) and search for a sentiment model.\n",
        "\n",
        "Can you find one for reviews, movies, or something else that fits the problem well?\n",
        "\n",
        "Only use models that have the following:\n",
        "- Have summary statistics of its performance on a test or validation dataset  \n",
        "- Have a python API (you should see a window for it at the bottom of the page)\n",
        "- Take **RAW TEXT** (not the vectorized words) as input into the model\n",
        "- Output **POSITIVE** or **NEGATIVE** (if its output is pos/neg/neutral, this is fine, but you will need to transform this output to only pos/neg and described how you handle **neutral** outputs)\n",
        "\n",
        "Report the url of the model's page you found in the box below.\n",
        "\n",
        "In the code cell below, evaluate the performance of your chosen pre-trained model on the v2.0 polarity dataset. What's the accuracy, F1 Score, and other metrics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_WgMZvm8d1a3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####URL TO MODEL PAGE: _________________\n",
        "####Hugging Face Reported TEST/VALIDATION PERFORMANCE: _________________\n",
        "####v2.0 Polarity Dataset TEST PERFORMANCE: _________________"
      ],
      "metadata": {
        "id": "tJGyHJjLm8uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "_aZ-nXthp-C4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n"
      ],
      "metadata": {
        "id": "Yns9uS__fZZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Using A PyTorch Model From Hugging Face\n",
        "\n",
        "Here, we will be using a movie sentiment model made by JamesH. The model card can be found [here](JamesH/Movie_review_sentiment_analysis_model).\n",
        "\n",
        "Report the url of the model's page you found in the box below.\n",
        "\n",
        "In the code cells below, evaluate the performance of your chosen pre-trained model on the v2.0 polarity dataset. What's the accuracy, F1 Score, and other metrics?"
      ],
      "metadata": {
        "id": "Dkd-VUIes7_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####URL TO MODEL PAGE: _________________\n",
        "####Hugging Face Reported TEST/VALIDATION PERFORMANCE: _________________\n",
        "####v2.0 Polarity Dataset TEST PERFORMANCE: _________________"
      ],
      "metadata": {
        "id": "-cl7ROCrthjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Loading the model"
      ],
      "metadata": {
        "id": "ArE6GS1NtyKi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LaDYg8XNQx5y"
      },
      "outputs": [],
      "source": [
        "james_h_model = AutoModelForSequenceClassification.from_pretrained(\"JamesH/autotrain-third-project-1883864250\")\n",
        "james_h_tokenizer = AutoTokenizer.from_pretrained(\"JamesH/autotrain-third-project-1883864250\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Example API of how to interface with the JamesH model"
      ],
      "metadata": {
        "id": "ttsQ177-tsnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_phrase = \"That movie wasn't good. For several reasons. Firstly, there isn't anything special about it\"\n",
        "with torch.no_grad():\n",
        "  inputs = james_h_tokenizer(input_phrase, return_tensors=\"pt\")\n",
        "  outputs = james_h_model(**inputs)\n",
        "  print(outputs)\n",
        "  # Output is a tuple containing the logits of the positive and negative predictions\n",
        "  # We need to convert these to probabilities\n",
        "  logits = outputs[\"logits\"]\n",
        "  odds = torch.exp(logits)\n",
        "  probabilities = odds / (1 + odds)\n",
        "  print(probabilities)\n",
        "  # Lets make it look more readable\n",
        "  positive = probabilities[0,0]\n",
        "  negative = probabilities[0,1]\n",
        "  print(\"\\n\")\n",
        "  print(f\"Phrase: {input_phrase}\\nPositive: {positive:.3f}\\nNegative: {negative:.3f}\")\n",
        "\n",
        "  if positive > 0.5:\n",
        "    print(f\"The phrase has positive sentiment\")\n",
        "  else:\n",
        "    print(f\"The phrase has negative sentiment\")"
      ],
      "metadata": {
        "id": "Jgy91zHuLOPe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "92869456-2167-4eb4-d903-3f5ef6fc5226"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-99c70867b5ab>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"That movie wasn't good. For several reasons. Firstly, there isn't anything special about it\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjames_h_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_phrase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjames_h_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'james_h_tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary"
      ],
      "metadata": {
        "id": "FT7YoS8jt6OC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Fine-tuning a Pre-Trained Model On Our Dataset\n",
        "\n",
        "Here, we will be using the JamesH model as the starting point of training a new model that's trained specifically for our dataset\n",
        "\n",
        "You will be tasked with finding hyperparameters to use during the fine-tuning process that improves the original model's performance.\n",
        "\n",
        "Then you will report the new model performance on the fine-tuned model."
      ],
      "metadata": {
        "id": "ZrfQ8mbKeJlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import datasets\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "df_train = pd.DataFrame(columns=['label', 'text'])\n",
        "df_test = pd.DataFrame(columns=['label', 'text'])\n",
        "\n",
        "pos_reviews = []\n",
        "neg_reviews = []\n",
        "\n",
        "# Getting all positive reviews from disk\n",
        "for filename in os.listdir(\"./txt_sentoken/pos\"):\n",
        "    with open(f\"./txt_sentoken/pos/{filename}\", \"r\") as f:\n",
        "        pos_reviews.append(f.read())\n",
        "\n",
        "# Getting all negative reviews from disk\n",
        "for filename in os.listdir(\"./txt_sentoken/neg\"):\n",
        "    with open(f\"./txt_sentoken/neg/{filename}\", \"r\") as f:\n",
        "        neg_reviews.append(f.read())\n",
        "\n",
        "# Randomly shuffle both lists for splitting into training and test sets\n",
        "random.shuffle(pos_reviews)\n",
        "random.shuffle(neg_reviews)\n",
        "\n",
        "# Add each review into the dataset variable\n",
        "# This dataset format is compatible for most Hugging Face wrappers in PyTorch\n",
        "test_percentage = 0.2\n",
        "\n",
        "for i in range(len(neg_reviews)):\n",
        "  temp = {}\n",
        "  temp[\"label\"] = 0\n",
        "  temp[\"text\"] = neg_reviews[i]\n",
        "  temp = pd.DataFrame(temp, index=[0])\n",
        "  if i < int(len(neg_reviews) * test_percentage):\n",
        "    df_test = pd.concat([df_test, temp], ignore_index = True)\n",
        "    df_test.reset_index()\n",
        "  else:\n",
        "    df_train = pd.concat([df_train, temp], ignore_index = True)\n",
        "    df_train.reset_index()\n",
        "\n",
        "for i in range(len(pos_reviews)):\n",
        "  temp = {}\n",
        "  temp[\"label\"] = 1\n",
        "  temp[\"text\"] = pos_reviews[i]\n",
        "  temp = pd.DataFrame(temp, index=[0])\n",
        "  if i < int(len(pos_reviews) * test_percentage):\n",
        "    df_test = pd.concat([df_test, temp], ignore_index = True)\n",
        "    df_test.reset_index()\n",
        "  else:\n",
        "    df_train = pd.concat([df_train, temp], ignore_index = True)\n",
        "    df_train.reset_index()\n",
        "\n",
        "\n",
        "dataset_train = datasets.Dataset.from_pandas(df_train)\n",
        "dataset_test = datasets.Dataset.from_pandas(df_test)\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return james_h_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_train = dataset_train.map(tokenize_function, batched=True)\n",
        "tokenized_test = dataset_test.map(tokenize_function, batched=True)\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "rpWxbWgcVFNR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "a3e2cbf0-8af0-434d-cf5d-1dbfec246920"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-dd86dbeb4b3f>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Getting all positive reviews from disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./txt_sentoken/pos\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"./txt_sentoken/pos/{filename}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpos_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './txt_sentoken/pos'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### See the [documentation](https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TrainingArguments) what different hyperparameters can be adjusted for the fine-tuning.\n",
        "\n",
        "### Play around with different parameters, specifically with:\n",
        "- learning rate\n",
        "- weight decay\n",
        "- max gradient norm\n",
        "- adam epsilon\n",
        "- num_train_epochs\n",
        "\n",
        "\n",
        "What happens when you change them? Does the fine-tuning take shorter? Longer? Does the performance improve or worsen? Try a few different configurations, record the results, and hypothesize (or if you can, explain!) the reason why theses hyperparameter changes had the observed effect.  \n",
        "\n",
        "*Note: Do NOT change the **output_dir** or **use_cpu** or **evalulation_strategy** variables.*  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QhwUIynSmXWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n",
        "#   See https://huggingface.co/transformers/v3.0.2/main_classes/trainer.html#transformers.TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=\"finetuned_movie_sentiment\", use_cpu=False, evaluation_strategy=\"epoch\", )\n",
        "\n",
        "#----------------------------------------------\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=james_h_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "SYC3wL5dl9Hz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the code cell below, evaluate the performance of your chosen pre-trained model on the v2.0 polarity dataset. What's the accuracy, F1 Score, and other metrics."
      ],
      "metadata": {
        "id": "f3bfNPEFMxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "james_h_finetuned_model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned_movie_sentiment/checkpoint-500\")\n",
        "\n",
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary\n"
      ],
      "metadata": {
        "id": "22FUzyxfLJFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G65MbRfQx5y"
      },
      "source": [
        "## Problem 5 (10 points): Accuracy is not everything!  How fast are the algorithms versus their accuracy?\n",
        "**Compare the runtime of your  baseline algorithms to the runtime of the pre-trained Hugging Face model and the fine-tuned Hugging Face model**\n",
        "\n",
        "**The jupyter command %timeit can be used to measure how long a calculation takes https://ipython.readthedocs.io/en/stable/interactive/magics.html.**\n",
        "* How long does it take to run the \"predict\" function on the entirety of v2.0 polarity dataset on the Sci-Kit Learn models and the Hugging Face models? Can you explain why? Make a table showing your results.\n",
        "* Which method has the best ability in predicting the sentiment correctly? Can you explain why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smz1f8tyQtfX"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------\n",
        "# Your code starts here\n",
        "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
        "#   Please feel free to add more cells below this cell if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "o0bcrrUzQtfX"
      },
      "source": [
        "\n",
        "## Problem 6 (20 points): Business question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8VZdOxfQtfX"
      },
      "source": [
        "* Suppose you had a machine learning algorithm that could detect the sentiment of tweets that was highly accurate.  What kind of business could you build around that?\n",
        "* Who would be your competitors, and what are their sizes?\n",
        "* What would be the size of the market for your product?\n",
        "* In addition, assume that your machine learning was slow to train, but fast in making predictions on new data.  How would that affect your business plan?\n",
        "* How could you use the cloud to support your product?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-g3psMyQtfX"
      },
      "outputs": [],
      "source": [
        "#A potential business that could be built with such an algorithm is to predict the success/failure of a celebrity's endeavors based\n",
        "#on the public's opinion of them. For example, if an artist were to drop an album, would it be a success or a flop? Do people care\n",
        "#about this celebrity? If the public is neutral, no one cares enough to listen. Negative sentiment may lead to boycotts and such, but\n",
        "#positive will mean success for the artist.\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhHQoH_KQtfX"
      },
      "source": [
        "# Slides (for a 5-8 minute presentation) (20 points)\n",
        "\n",
        "\n",
        "1. (5 points) Motivation about the data collection, why the topic is interesting to you.\n",
        "\n",
        "2. (10 points) Communicating Results (figure/table)\n",
        "\n",
        "3. (5 points) Story telling (How all the parts (data, analysis, result) fit together as a story?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl5sy8EUQtfY"
      },
      "source": [
        "\n",
        "# Done\n",
        "\n",
        "All set!\n",
        "\n",
        "** What do you need to submit?**\n",
        "\n",
        "* **Notebook File**: Save this IPython notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"ipython notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
        "\n",
        "\n",
        "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . We will ask two teams which are randomly selected to present their case studies in class for this case study.\n",
        "\n",
        "* **Report**: please prepare a report (less than 10 pages) to report what you found in the data.\n",
        "    * What data you collected?\n",
        "    * Why this topic is interesting or important to you? (Motivations)\n",
        "    * How did you analyse the data?\n",
        "    * What did you find in the data?\n",
        "\n",
        "     (please include figures or tables in the report, but no source code)\n",
        "\n",
        "\n",
        "*Please compress all the files into a single zipped file.*\n",
        "\n",
        "\n",
        "** How to submit: **\n",
        "\n",
        "        Please submit through canvas.wpi.edu\n",
        "\n",
        "### DS3010 Case Study 3 Team ??\n",
        "\n",
        "#### where ?? is your team number.\n",
        "        \n",
        "** Note: Each team just needs to submits one submission **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "HylAvN1gQtfY"
      },
      "source": [
        "# Grading Criteria:\n",
        "\n",
        "**Total Points: 100**\n",
        "\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Notebook results:**\n",
        "    Points: 80\n",
        "\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 1:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "    \n",
        "    -----------------------------------\n",
        "    Question 2:\n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "        \n",
        "    -----------------------------------\n",
        "    Question 3:\n",
        "    Points: 15\n",
        "    -----------------------------------\n",
        "  \n",
        "    -----------------------------------\n",
        "    Question 4:  \n",
        "    Points: 15\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 5:  \n",
        "    Points: 10\n",
        "    -----------------------------------\n",
        "\n",
        "    -----------------------------------\n",
        "    Question 6:  \n",
        "    Points: 20\n",
        "    -----------------------------------\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "**Slides (for a 5-8 minute presentation): Story-telling**\n",
        "    Points: 20\n",
        "\n",
        "\n",
        "1. Motivation about the data collection, why the topic is interesting to you.\n",
        "    Points: 5\n",
        "\n",
        "2. Communicating Results (figure/table)\n",
        "    Points: 10\n",
        "\n",
        "3. Story telling (How all the parts (data, analysis, result) fit together as a story?)\n",
        "    Points: 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch582GFeQtfY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "0.2.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}